{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBALayer \n",
    "\n",
    "This notebook shows an example of the [IBALayer](https://github.com/albermax/innvestigate) for CIFAR-10.\n",
    "If you cannot add an additional layer to your network, have a look at [IBACopyGraph]() and [IBACopyGraphInnvestigate]().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to set you cuda device\n",
    "# %env CUDA_VISIBLE_DEVICES=1\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# reduce tensorflow noise\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "\n",
    "#from skimage.transform import resize\n",
    "\n",
    "from IBA.utils import plot_saliency_map\n",
    "from IBA.tensorflow_v1 import IBALayer, model_wo_softmax, to_saliency_map \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow version: {}, Keras version: {}\".format(\n",
    "    tf.version.VERSION, keras.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "\n",
    "# set True to train the model yourself\n",
    "run_training = True\n",
    "run_training = False\n",
    "\n",
    "model_weight_url =  \"https://userpage.fu-berlin.de/leonsixt/cifar_weights.h5\"\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download model weights\n",
    "    \n",
    "! wget -O 'cifar_weights.h5' {model_weight_url} \n",
    "! echo 37d997479fbe7a0282dd0808cc1d286eb481b23279837c9ccfbec989d209eea4  cifar_weights.h5 > check_sha256sum.txt\n",
    "! sha256sum --check check_sha256sum.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', name='conv1', use_bias=False,\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization(name='bn1'))\n",
    "model.add(Activation('relu', name='relu1'))\n",
    "\n",
    "model.add(Conv2D(64, (5, 5), padding='same', name='conv2', use_bias=False))\n",
    "model.add(BatchNormalization(name='bn2'))\n",
    "model.add(Activation('relu', name='relu2'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), name='pool2'))    #  8\n",
    "\n",
    "# just add IBALayer where you want to explain the model.\n",
    "# you could even add multiple IBALayers to a single model.\n",
    "model.add(IBALayer())\n",
    "model.add(Conv2D(128, (5, 5), padding='same', name='conv3', use_bias=False))\n",
    "model.add(BatchNormalization(name='bn3'))\n",
    "model.add(Activation('relu', name='relu3'))\n",
    "\n",
    "model.add(Conv2D(128, (5, 5), padding='same', name='conv4', use_bias=False))\n",
    "model.add(Activation('relu', name='relu4'))\n",
    "model.add(GlobalAveragePooling2D(name='pool4'))   \n",
    "\n",
    "model.add(Dropout(0.5, name='dropout1'))\n",
    "model.add(Dense(1024, name='fc1'))\n",
    "model.add(Activation('relu', name='relu5'))\n",
    "\n",
    "model.add(Dropout(0.5, name='dropout2'))\n",
    "model.add(Dense(num_classes, name='fc2'))\n",
    "model.add(Activation('softmax', name='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not run_training:\n",
    "    print(\"loading weights\")\n",
    "    model.load_weights('cifar_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_training: \n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=20,  \n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "    )\n",
    "\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    hist = model.fit_generator(\n",
    "        datagen.flow(x_train, y_train, batch_size=batch_size), \n",
    "        epochs=epochs, \n",
    "        steps_per_epoch=len(x_train) // batch_size,\n",
    "        validation_data=(x_test, y_test), workers=4)\n",
    "    \n",
    "    model.save_weights(\"cifar_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logits = model_wo_softmax(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iba = model.layers[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = iba.set_classification_loss(model_logits.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure model is in eval mode\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "# estimate mean, std on 5000 samples\n",
    "for img in tqdm(x_train[:5000]):\n",
    "    iba.fit({model.input: img[None]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 2\n",
    "cols = 6\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(2.5*cols, 2.*rows))\n",
    "\n",
    "for i, ax0, ax1 in zip(range(rows*cols//2), ax.flatten()[::2], ax.flatten()[1::2]):\n",
    "    img = x_test[i:i+1]\n",
    "    ax0.imshow(img[0])\n",
    "    ax0.set_xticks([])\n",
    "    ax0.set_yticks([])\n",
    "    target = y_test[i].nonzero()[0]\n",
    "    capacity = iba.analyze({model.input: img, iba.target: target})\n",
    "    saliency_map = to_saliency_map(capacity, shape=(32, 32))\n",
    "\n",
    "    plot_saliency_map(saliency_map, ax=ax1, colorbar_size=0.15, colorbar_fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access to internal values \n",
    "\n",
    "You can access all intermediate values of the optimzation through the `iba.get_report()` method.\n",
    "To store the intermediate values, you have to call either `iba.collect_all()` or `iba.collect(*var_names)` before running `iba.analyze(..)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all intermediate tensors\n",
    "iba.collect_all()\n",
    "\n",
    "# storing all tensors can slow down the optimization. \n",
    "# you can also select to store only specific ones:\n",
    "# iba.collect(\"alpha\", \"model_loss\")\n",
    "# to only collect a subset all all tensors\n",
    "\n",
    "# run analyze\n",
    "\n",
    "i = 4\n",
    "img = x_test[i][None]\n",
    "target = y_test[i].nonzero()[0]\n",
    "capacity = iba.analyze({model.input: img, iba.target: target})\n",
    "saliency_map = to_saliency_map(capacity, shape=(32, 32))\n",
    "\n",
    "# get all saved variables\n",
    "report = iba.get_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`report` is an `OrderedDict`  which maps each `iteration` to a dictionray of `{var_name, var_value}`.\n",
    "The `init` iteration is computed without an optimizer update. Values not changing such as the feature values are only included in the `init` iteration.\n",
    "The `final` iteration is again computed without an optimizer update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"iterations:\", list(report.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Print all available tensors in the `init` iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:<30} {:}\".format(\"name:\", \"shape\"))\n",
    "print()\n",
    "for name, val in report['init'].items():\n",
    "    print(\"{:<30} {:}\".format(name + \":\", str(val.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses during optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax[0].set_title(\"cross entrop loss\")\n",
    "ax[0].plot(list(report.keys()), [it['model_loss'] for it in report.values()])\n",
    "\n",
    "ax[1].set_title(\"mean capacity\")\n",
    "ax[1].plot(list(report.keys()), [it['capacity_mean'] for it in report.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of alpha (pre-softmax) values per iteraton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 6\n",
    "rows = len(report) // cols\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(2.8*cols, 2.2*rows))\n",
    "\n",
    "for ax, (it, values) in zip(axes.flatten(), report.items()):\n",
    "    ax.hist(values['alpha'].flatten(), log=True, bins=20)\n",
    "    ax.set_title(\"iteration: \" + str(it))\n",
    "    \n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.5)\n",
    "\n",
    "fig.suptitle(\"distribution of alpha (pre-softmax) values per iteraton.\", y=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributiuon of the final capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(report['final']['capacity'].flatten(), bins=20, log=True)\n",
    "plt.title(\"Distributiuon of the final capacity\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfv1]",
   "language": "python",
   "name": "conda-env-tfv1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
